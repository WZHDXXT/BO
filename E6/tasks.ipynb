{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint BO\n",
    "\n",
    "You are given code for SCBO -- TurBO with constraints. Read and see what is going on.\n",
    "The code is optimizing Ackley function.\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "Create convergence plot on Ackley function. If `fun` is one of `botorch.test_functions`, then the best values found for every function evaluation are provided in `fun.optimal_value`.\n",
    "\n",
    "#### Task 2 \n",
    "\n",
    "Choose 5 functions from `botorch.test_functions` and optimize them using this algorithm.\n",
    "\n",
    "### Task 3 \n",
    "\n",
    "Run any other algorithm of your choice that can handle constrants and compare the results against SCBO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gpytorch\n",
    "import torch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "# Constrained Max Posterior Sampling s a new sampling class, similar to MaxPosteriorSampling,\n",
    "# which implements the constrained version of Thompson Sampling described in [1].\n",
    "from botorch.generation.sampling import ConstrainedMaxPosteriorSampling\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "dtype = torch.double\n",
    "tkwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "\n",
    "# Here we define the example 10D Ackley function\n",
    "fun = Ackley(dim=10, negate=True).to(**tkwargs)\n",
    "\n",
    "fun.bounds[0, :].fill_(-5)\n",
    "fun.bounds[1, :].fill_(10)\n",
    "\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "\n",
    "batch_size = 4\n",
    "n_init = 10\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "\n",
    "# When evaluating the function, we must first unnormalize the inputs since\n",
    "# we will use normalized inputs x in the main optimizaiton loop\n",
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))\n",
    "\n",
    "def c1(x):  # Equivalent to enforcing that sum(x) <= 0\n",
    "    return x.sum()\n",
    "def c2(x):  # Equivalent to enforcing that ||x||_2 <= 5\n",
    "    return torch.norm(x, p=2) - 5\n",
    "# We assume c1, c2 have same bounds as the Ackley function above\n",
    "def eval_c1(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return c1(unnormalize(x, fun.bounds))\n",
    "def eval_c2(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return c2(unnormalize(x, fun.bounds))\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5**7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    best_constraint_values: Tensor = torch.ones(2, **tkwargs) * torch.inf\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(max([4.0 / self.batch_size, float(self.dim) / self.batch_size]))\n",
    "\n",
    "\n",
    "def update_tr_length(state: ScboState):\n",
    "    # Update the length of the trust region according to\n",
    "    # success and failure counters\n",
    "    # (Just as in original TuRBO paper)\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    if state.length < state.length_min:  # Restart when trust region becomes too small\n",
    "        state.restart_triggered = True\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_best_index_for_batch(Y: Tensor, C: Tensor):\n",
    "    \"\"\"Return the index for the best point.\"\"\"\n",
    "    is_feas = (C <= 0).all(dim=-1)\n",
    "    if is_feas.any():  # Choose best feasible candidate\n",
    "        score = Y.clone()\n",
    "        score[~is_feas] = -float(\"inf\")\n",
    "        return score.argmax()\n",
    "    return C.clamp(min=0).sum(dim=-1).argmin()\n",
    "\n",
    "\n",
    "def update_state(state, Y_next, C_next):\n",
    "    \"\"\"Method used to update the TuRBO state after each step of optimization.\n",
    "\n",
    "    Success and failure counters are updated according to the objective values\n",
    "    (Y_next) and constraint values (C_next) of the batch of candidate points\n",
    "    evaluated on the optimization step.\n",
    "\n",
    "    As in the original TuRBO paper, a success is counted whenver any one of the\n",
    "    new candidate points improves upon the incumbent best point. The key difference\n",
    "    for SCBO is that we only compare points by their objective values when both points\n",
    "    are valid (meet all constraints). If exactly one of the two points being compared\n",
    "    violates a constraint, the other valid point is automatically considered to be better.\n",
    "    If both points violate some constraints, we compare them inated by their constraint values.\n",
    "    The better point in this case is the one with minimum total constraint violation\n",
    "    (the minimum sum of constraint values)\"\"\"\n",
    "\n",
    "    # Pick the best point from the batch\n",
    "    best_ind = get_best_index_for_batch(Y=Y_next, C=C_next)\n",
    "    y_next, c_next = Y_next[best_ind], C_next[best_ind]\n",
    "\n",
    "    if (c_next <= 0).all():\n",
    "        # At least one new candidate is feasible\n",
    "        improvement_threshold = state.best_value + 1e-3 * math.fabs(state.best_value)\n",
    "        if y_next > improvement_threshold or (state.best_constraint_values > 0).any():\n",
    "            state.success_counter += 1\n",
    "            state.failure_counter = 0\n",
    "            state.best_value = y_next.item()\n",
    "            state.best_constraint_values = c_next\n",
    "        else:\n",
    "            state.success_counter = 0\n",
    "            state.failure_counter += 1\n",
    "    else:\n",
    "        # No new candidate is feasible\n",
    "        total_violation_next = c_next.clamp(min=0).sum(dim=-1)\n",
    "        total_violation_center = state.best_constraint_values.clamp(min=0).sum(dim=-1)\n",
    "        if total_violation_next < total_violation_center:\n",
    "            state.success_counter += 1\n",
    "            state.failure_counter = 0\n",
    "            state.best_value = y_next.item()\n",
    "            state.best_constraint_values = c_next\n",
    "        else:\n",
    "            state.success_counter = 0\n",
    "            state.failure_counter += 1\n",
    "\n",
    "    # Update the length of the trust region according to the success and failure counters\n",
    "    state = update_tr_length(state)\n",
    "    return state\n",
    "\n",
    "\n",
    "# Define example state\n",
    "state = ScboState(dim=dim, batch_size=batch_size)\n",
    "print(state)\n",
    "\n",
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init\n",
    "\n",
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    C,  # Constraint values\n",
    "    batch_size,\n",
    "    n_candidates,  # Number of candidates for Thompson sampling\n",
    "    constraint_model,\n",
    "    sobol: SobolEngine,\n",
    "):\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "\n",
    "    # Create the TR bounds\n",
    "    best_ind = get_best_index_for_batch(Y=Y, C=C)\n",
    "    x_center = X[best_ind, :].clone()\n",
    "    tr_lb = torch.clamp(x_center - state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    # Thompson Sampling w/ Constraints (SCBO)\n",
    "    dim = X.shape[-1]\n",
    "    pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "    pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "    # Create a perturbation mask\n",
    "    prob_perturb = min(20.0 / dim, 1.0)\n",
    "    mask = torch.rand(n_candidates, dim, **tkwargs) <= prob_perturb\n",
    "    ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "    mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "    # Create candidate points from the perturbations and the mask\n",
    "    X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "    X_cand[mask] = pert[mask]\n",
    "\n",
    "    # Sample on the candidate points using Constrained Max Posterior Sampling\n",
    "    constrained_thompson_sampling = ConstrainedMaxPosteriorSampling(\n",
    "        model=model, constraint_model=constraint_model, replacement=False\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        X_next = constrained_thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "    return X_next\n",
    "\n",
    "\n",
    "# Generate initial data\n",
    "train_X = get_initial_points(dim, n_init)\n",
    "train_Y = torch.tensor([eval_objective(x) for x in train_X], **tkwargs).unsqueeze(-1)\n",
    "C1 = torch.tensor([eval_c1(x) for x in train_X], **tkwargs).unsqueeze(-1)\n",
    "C2 = torch.tensor([eval_c2(x) for x in train_X], **tkwargs).unsqueeze(-1)\n",
    "\n",
    "# Initialize TuRBO state\n",
    "state = ScboState(dim, batch_size=batch_size)\n",
    "\n",
    "# Note: We use 2000 candidates here to make the tutorial run faster.\n",
    "# SCBO actually uses min(5000, max(2000, 200 * dim)) candidate points by default.\n",
    "N_CANDIDATES = 2000 if not SMOKE_TEST else 4\n",
    "sobol = SobolEngine(dim, scramble=True, seed=1)\n",
    "\n",
    "\n",
    "def get_fitted_model(X, Y):\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0))\n",
    "    )\n",
    "    model = SingleTaskGP(\n",
    "        X,\n",
    "        Y,\n",
    "        covar_module=covar_module,\n",
    "        likelihood=likelihood,\n",
    "        outcome_transform=Standardize(m=1),\n",
    "    )\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        fit_gpytorch_mll(mll)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "    # Fit GP models for objective and constraints\n",
    "    model = get_fitted_model(train_X, train_Y)\n",
    "    c1_model = get_fitted_model(train_X, C1)\n",
    "    c2_model = get_fitted_model(train_X, C2)\n",
    "\n",
    "    # Generate a batch of candidates\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        X_next = generate_batch(\n",
    "            state=state,\n",
    "            model=model,\n",
    "            X=train_X,\n",
    "            Y=train_Y,\n",
    "            C=torch.cat((C1, C2), dim=-1),\n",
    "            batch_size=batch_size,\n",
    "            n_candidates=N_CANDIDATES,\n",
    "            constraint_model=ModelListGP(c1_model, c2_model),\n",
    "            sobol=sobol,\n",
    "        )\n",
    "\n",
    "    # Evaluate both the objective and constraints for the selected candidaates\n",
    "    Y_next = torch.tensor([eval_objective(x) for x in X_next], dtype=dtype, device=device).unsqueeze(-1)\n",
    "    C1_next = torch.tensor([eval_c1(x) for x in X_next], dtype=dtype, device=device).unsqueeze(-1)\n",
    "    C2_next = torch.tensor([eval_c2(x) for x in X_next], dtype=dtype, device=device).unsqueeze(-1)\n",
    "    C_next = torch.cat([C1_next, C2_next], dim=-1)\n",
    "\n",
    "    # Update TuRBO state\n",
    "    state = update_state(state=state, Y_next=Y_next, C_next=C_next)\n",
    "\n",
    "    # Append data. Note that we append all data, even points that violate\n",
    "    # the constraints. This is so our constraint models can learn more\n",
    "    # about the constraint functions and gain confidence in where violations occur.\n",
    "    train_X = torch.cat((train_X, X_next), dim=0)\n",
    "    train_Y = torch.cat((train_Y, Y_next), dim=0)\n",
    "    C1 = torch.cat((C1, C1_next), dim=0)\n",
    "    C2 = torch.cat((C2, C2_next), dim=0)\n",
    "\n",
    "    # Print current status. Note that state.best_value is always the best\n",
    "    # objective value found so far which meets the constraints, or in the case\n",
    "    # that no points have been found yet which meet the constraints, it is the\n",
    "    # objective value of the point with the minimum constraint violation.\n",
    "    if (state.best_constraint_values <= 0).all():\n",
    "        print(f\"{len(train_X)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\")\n",
    "    else:\n",
    "        violation = state.best_constraint_values.clamp(min=0).sum()\n",
    "        print(\n",
    "            f\"{len(train_X)}) No feasible point yet! Smallest total violation: \"\n",
    "            f\"{violation:.2e}, TR length: {state.length:.2e}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
